# **Disaster-Pipeline**
- Introduction
- Data Sources
- Outcomes

## 1. Introduction

This is one of the projects in the Udacity Nanodegree program for Data Science. This is based upon tweets from disaster situations. The main aim of the project was to use NLTK (Natural Language Process libraries) to break messages into important words and categories. Initially the data needed cleaning and was put through the Extraction, Transform and Load process (ETL). After this, it needed to go through a machine learning process involving NLTK. Finally, it was then adapted into an app.

## 2. Data Sources ##
 - Process_data.py: This takes a csv file, cleans it and creates a SQL database from it.
 - train_classifier.py: This code trains the machine learning model with the SQL database
 - ETL Pipline Preparation.ipbynb: This the Jupyter Notebook involved with process_data.py 
 - ML Pipeline Preparation.ipynb: This is the Jupyter Notebook involved with the development process
 - data: the folder that contains the messages and categories datasets in csv format
 - app: folder that contains the run.py to initiate the web app
 
 ## 3. Outcomes ##


